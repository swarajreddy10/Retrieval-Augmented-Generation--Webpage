# ğŸ“„ Ask the Docs â€“ Retrieval-Augmented Generation (RAG) Web App

Ask the Docs is a fully functional, intelligent, and responsive RAG-based mini web application that allows users to upload documents and ask context-aware questions. Built as part of a Cloud & Backend Engineering internship challenge, the system extracts, embeds, and retrieves relevant data to generate accurate and informative answers using a free, open-source LLM.

## ğŸš€ Features

- Upload PDF or text files
- Chunk & embed documents into vector format
- Semantic search using FAISS
- Context-aware question answering using a free open-source LLM
- Intuitive UI powered by Streamlit
- Deployed on Render and AWS EC2 with Elastic IP for 24/7 accessibility

---

## ğŸ› ï¸ Tech Stack & Tools

### ğŸ§  Backend Logic
- **LangChain** â€“ Simplified RAG pipeline
- **SentenceTransformers** â€“ `all-MiniLM-L6-v2` for high-speed, low-latency embeddings
- **Transformers** â€“ Hugging Face Transformers for model inference
- **FAISS** â€“ Facebook AI Similarity Search for efficient vector storage and retrieval
- **PyPDF2** â€“ Extract text from PDF documents
- **dotenv & requests** â€“ For secure environment variable handling and API communication

### ğŸ’» Frontend & Deployment
- **Streamlit** â€“ UI framework for interactive web interface
- **Python 3.8+**
- **Render** â€“ For quick public deployment
- **AWS EC2 with Elastic IP** â€“ Scalable backend hosting with persistent public access

---

## ğŸ¤– Free LLM Used

- `mistralai/Mistral-7B-Instruct-v0.1` *(or)* `tiiuae/falcon-7b-instruct` *(or)* `HuggingFaceH4/zephyr-7b-beta`
- Hosted via Hugging Face Inference API
- **No API key required** under free tier for limited usage

These LLMs are instruction-tuned and support English Q&A with high performance and relatively low memory use, ideal for RAG workflows.

---

## âš™ï¸ Workflow

1. **Upload Document**: User selects a PDF or text file.
2. **Text Extraction & Chunking**: Large documents are split into chunks for processing.
3. **Embedding**: Each chunk is converted into a vector using `all-MiniLM-L6-v2`.
4. **Storage in FAISS**: Vectors are stored in a searchable vector database.
5. **User Query Input**: Natural language question is entered by the user.
6. **Relevant Context Retrieval**: FAISS returns top-matching chunks.
7. **Answer Generation**: An LLM (Mistral or Falcon) generates a precise answer using retrieved context.

---

## ğŸŒ Deployment

This app is hosted on:
- **DEMO Video Drive Link** -  https://drive.google.com/file/d/1K4QhtA9cv68eZOjmklLlcu5CWqM_Ztif/view?usp=sharing
- **Render** â€“ https://rag-app-w9pz.onrender.com
- **AWS EC2 with Elastic IP** â€“ Provides persistent and scalable deployment with public access. The instance runs a production-grade backend configured with Gunicorn and Nginx for better performance and uptime.

---

## ğŸ“¦ Installation (Local)

```bash
git clone https://github.com/Your-name/ASKtheDOCS_Retrieval-Augmented-Generation
cd ASKtheDOCS_Retrieval-Augmented-Generation
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
streamlit run app.py
